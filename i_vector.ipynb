{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:WARNNG: libsvm is not installed, please refer to the documentation if you intend to use SVM classifiers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from multiprocessing import cpu_count\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import sidekit\n",
    "from model_interface import SidekitModel\n",
    "from ubm import UBM\n",
    "\n",
    "from utils import parse_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf_path:  conf.yaml\n",
      "NUM_GAUSSIANS:  64\n",
      "BATCH_SIZE:  30\n",
      "TV_RANK:  25\n",
      "TV_ITERATIONS:  50\n",
      "ENABLE_PLDA:  none\n",
      "BASE_DIR:  C:/Users/sean/han_ma_eum/Speaker-Recognition/outputs\n"
     ]
    }
   ],
   "source": [
    "conf_path = \"conf.yaml\"\n",
    "\n",
    "conf = parse_yaml(conf_path)\n",
    "NUM_GAUSSIANS = conf['num_gaussians']\n",
    "BATCH_SIZE = conf['batch_size']\n",
    "TV_RANK = conf['tv_rank']\n",
    "TV_ITERATIONS = conf['tv_iterations']\n",
    "ENABLE_PLDA = conf['enable_plda']\n",
    "BASE_DIR = conf['outpath']\n",
    "\n",
    "print(\"conf_path: \", conf_path)\n",
    "print(\"NUM_GAUSSIANS: \", NUM_GAUSSIANS)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"TV_RANK: \", TV_RANK)\n",
    "print(\"TV_ITERATIONS: \", TV_ITERATIONS)\n",
    "print(\"ENABLE_PLDA: \", ENABLE_PLDA)\n",
    "print(\"BASE_DIR: \", BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading trained UBM-64 model\n",
      "INFO:root:Feature-Server is created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubm_name:  ubm_64.h5\n",
      "ubm_path:  C:/Users/sean/han_ma_eum/Speaker-Recognition/outputs\\ubm\\ubm_64.h5\n"
     ]
    }
   ],
   "source": [
    "# sidekitmodel = SidekitModel(conf_path)\n",
    "\n",
    "\n",
    "\"\"\"This private method is used to create Statistic Servers.TODO: post some more info\"\"\"\n",
    "\n",
    "# Read tv_idmap\n",
    "tv_idmap = sidekit.IdMap.read(os.path.join(BASE_DIR, \"task\", \"tv_idmap.h5\"))\n",
    "back_idmap = tv_idmap\n",
    "# If PLDA is enabled\n",
    "if ENABLE_PLDA:\n",
    "    # Read plda_idmap\n",
    "    plda_idmap = sidekit.IdMap.read(os.path.join(BASE_DIR, \"task\", \"plda_idmap.h5\"))\n",
    "    # Create a joint StatServer for TV and PLDA training data\n",
    "    back_idmap = plda_idmap.merge(tv_idmap)\n",
    "    if not back_idmap.validate():\n",
    "        raise RuntimeError(\"Error merging tv_idmap & plda_idmap\")\n",
    "\n",
    "# Check UBM model\n",
    "ubm_name = \"ubm_{}.h5\".format(NUM_GAUSSIANS)\n",
    "ubm_path = os.path.join(BASE_DIR, \"ubm\", ubm_name)\n",
    "print(\"ubm_name: \", ubm_name)\n",
    "print(\"ubm_path: \", ubm_path)\n",
    "if not os.path.exists(ubm_path):\n",
    "    print(\"train UBM...\")\n",
    "    #if UBM model does not exist, train one\n",
    "    logging.info(\"Training UBM-{} model\".format(NUM_GAUSSIANS))\n",
    "    ubm = UBM(conf_path)\n",
    "    ubm.train()\n",
    "#load trained UBM model\n",
    "logging.info(\"Loading trained UBM-{} model\".format(NUM_GAUSSIANS))\n",
    "ubm = sidekit.Mixture()\n",
    "ubm.read(ubm_path)\n",
    "back_stat = sidekit.StatServer( statserver_file_name=back_idmap, \n",
    "                                ubm=ubm\n",
    "                                )\n",
    "# Create Feature Server\n",
    "fs = SidekitModel(conf_path).createFeatureServer()\n",
    "\n",
    "# Jointly compute the sufficient statistics of TV and (if enabled) PLDA data\n",
    "back_filename = 'back_stat_{}.h5'.format(NUM_GAUSSIANS)\n",
    "if not os.path.isfile(os.path.join(BASE_DIR, \"stat\", back_filename)):\n",
    "    #BUG: don't use NUM_THREADS when assgining num_thread\n",
    "    # as it's prune to race-conditioning\n",
    "    back_stat.accumulate_stat(\n",
    "        ubm=ubm,\n",
    "        feature_server=fs,\n",
    "        seg_indices=range(back_stat.segset.shape[0])\n",
    "        )\n",
    "    back_stat.write(os.path.join(BASE_DIR, \"stat\", back_filename))\n",
    "\n",
    "# Load the sufficient statistics from TV training data\n",
    "tv_filename = 'tv_stat_{}.h5'.format(NUM_GAUSSIANS)\n",
    "if not os.path.isfile(os.path.join(BASE_DIR, \"stat\", tv_filename)):\n",
    "    tv_stat = sidekit.StatServer.read_subset(\n",
    "        os.path.join(BASE_DIR, \"stat\", back_filename),\n",
    "        tv_idmap\n",
    "        )\n",
    "    tv_stat.write(os.path.join(BASE_DIR, \"stat\", tv_filename))\n",
    "\n",
    "# Load sufficient statistics and extract i-vectors from PLDA training data\n",
    "if ENABLE_PLDA:\n",
    "    plda_filename = 'plda_stat_{}.h5'.format(NUM_GAUSSIANS)\n",
    "    if not os.path.isfile(os.path.join(BASE_DIR, \"stat\", plda_filename)):\n",
    "        plda_stat = sidekit.StatServer.read_subset(\n",
    "            os.path.join(BASE_DIR, \"stat\", back_filename),\n",
    "            plda_idmap\n",
    "            )\n",
    "        plda_stat.write(os.path.join(BASE_DIR, \"stat\", plda_filename))\n",
    "\n",
    "# Load sufficient statistics from test data\n",
    "filename = 'test_stat_{}.h5'.format(NUM_GAUSSIANS)\n",
    "if not os.path.isfile(os.path.join(BASE_DIR, \"stat\", filename)):\n",
    "    test_idmap = sidekit.IdMap.read(os.path.join(BASE_DIR, \"task\", \"test_idmap.h5\"))\n",
    "    test_stat = sidekit.StatServer( statserver_file_name=test_idmap, \n",
    "                                    ubm=ubm\n",
    "                                    )\n",
    "    # Create Feature Server\n",
    "    fs = SidekitModel(conf_path).createFeatureServer()\n",
    "    # Jointly compute the sufficient statistics of TV and PLDA data\n",
    "    #BUG: don't use NUM_THREADS when assgining num_thread as it's prune to race-conditioning\n",
    "    test_stat.accumulate_stat(ubm=ubm,\n",
    "                            feature_server=fs,\n",
    "                            seg_indices=range(test_stat.segset.shape[0])\n",
    "                            )\n",
    "    test_stat.write(os.path.join(BASE_DIR, \"stat\", filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UBM model\n",
    "model_name = \"ubm_{}.h5\".format(NUM_GAUSSIANS)\n",
    "ubm = sidekit.Mixture()\n",
    "ubm.read(os.path.join(BASE_DIR, \"ubm\", model_name))\n",
    "\n",
    "# Load TV matrix\n",
    "filename = \"tv_matrix_{}\".format(NUM_GAUSSIANS)\n",
    "outputPath = os.path.join(BASE_DIR, \"ivector\", filename)\n",
    "fa = sidekit.FactorAnalyser(outputPath+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Extracting i-vectors from enrollment data\n",
      "Processing: 100%|██████████| 300/300 [00:00<00:00, 913.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract i-vectors from enrollment data\n",
    "logging.info(\"Extracting i-vectors from enrollment data\")\n",
    "filename = 'enroll_stat_{}.h5'.format(NUM_GAUSSIANS)\n",
    "enroll_stat = sidekit.StatServer.read(os.path.join(BASE_DIR, 'stat', filename))\n",
    "enroll_iv = fa.extract_ivectors_single( ubm=ubm,\n",
    "                                        stat_server=enroll_stat,\n",
    "                                        uncertainty=False\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S01', 'S01', 'S01', 'S01', 'S01', 'S01', 'S02', 'S02', 'S02',\n",
       "       'S02', 'S02', 'S02', 'S03', 'S03', 'S03', 'S03', 'S03', 'S03',\n",
       "       'S04', 'S04', 'S04', 'S04', 'S04', 'S04', 'S05', 'S05', 'S05',\n",
       "       'S05', 'S05', 'S05', 'S06', 'S06', 'S06', 'S06', 'S06', 'S06',\n",
       "       'S07', 'S07', 'S07', 'S07', 'S07', 'S07', 'S08', 'S08', 'S08',\n",
       "       'S08', 'S08', 'S08', 'S09', 'S09', 'S09', 'S09', 'S09', 'S09',\n",
       "       'S10', 'S10', 'S10', 'S10', 'S10', 'S10', 'S11', 'S11', 'S11',\n",
       "       'S11', 'S11', 'S11', 'S12', 'S12', 'S12', 'S12', 'S12', 'S12',\n",
       "       'S13', 'S13', 'S13', 'S13', 'S13', 'S13', 'S14', 'S14', 'S14',\n",
       "       'S14', 'S14', 'S14', 'S15', 'S15', 'S15', 'S15', 'S15', 'S15',\n",
       "       'S16', 'S16', 'S16', 'S16', 'S16', 'S16', 'S17', 'S17', 'S17',\n",
       "       'S17', 'S17', 'S17', 'S18', 'S18', 'S18', 'S18', 'S18', 'S18',\n",
       "       'S19', 'S19', 'S19', 'S19', 'S19', 'S19', 'S20', 'S20', 'S20',\n",
       "       'S20', 'S20', 'S20', 'S21', 'S21', 'S21', 'S21', 'S21', 'S21',\n",
       "       'S22', 'S22', 'S22', 'S22', 'S22', 'S22', 'S23', 'S23', 'S23',\n",
       "       'S23', 'S23', 'S23', 'S24', 'S24', 'S24', 'S24', 'S24', 'S24',\n",
       "       'S25', 'S25', 'S25', 'S25', 'S25', 'S25', 'S26', 'S26', 'S26',\n",
       "       'S26', 'S26', 'S26', 'S27', 'S27', 'S27', 'S27', 'S27', 'S27',\n",
       "       'S28', 'S28', 'S28', 'S28', 'S28', 'S28', 'S29', 'S29', 'S29',\n",
       "       'S29', 'S29', 'S29', 'S30', 'S30', 'S30', 'S30', 'S30', 'S30',\n",
       "       'S31', 'S31', 'S31', 'S31', 'S31', 'S31', 'S32', 'S32', 'S32',\n",
       "       'S32', 'S32', 'S32', 'S33', 'S33', 'S33', 'S33', 'S33', 'S33',\n",
       "       'S34', 'S34', 'S34', 'S34', 'S34', 'S34', 'S35', 'S35', 'S35',\n",
       "       'S35', 'S35', 'S35', 'S36', 'S36', 'S36', 'S36', 'S36', 'S36',\n",
       "       'S37', 'S37', 'S37', 'S37', 'S37', 'S37', 'S38', 'S38', 'S38',\n",
       "       'S38', 'S38', 'S38', 'S39', 'S39', 'S39', 'S39', 'S39', 'S39',\n",
       "       'S40', 'S40', 'S40', 'S40', 'S40', 'S40', 'S41', 'S41', 'S41',\n",
       "       'S41', 'S41', 'S41', 'S42', 'S42', 'S42', 'S42', 'S42', 'S42',\n",
       "       'S43', 'S43', 'S43', 'S43', 'S43', 'S43', 'S44', 'S44', 'S44',\n",
       "       'S44', 'S44', 'S44', 'S45', 'S45', 'S45', 'S45', 'S45', 'S45',\n",
       "       'S46', 'S46', 'S46', 'S46', 'S46', 'S46', 'S47', 'S47', 'S47',\n",
       "       'S47', 'S47', 'S47', 'S48', 'S48', 'S48', 'S48', 'S48', 'S48',\n",
       "       'S49', 'S49', 'S49', 'S49', 'S49', 'S49', 'S50', 'S50', 'S50',\n",
       "       'S50', 'S50', 'S50'], dtype='<U255')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enroll_iv.modelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5311496 , -1.28135981,  1.32816947, ..., -0.64532526,\n",
       "        -0.79574252,  1.09020777],\n",
       "       [-1.05740162,  0.03279953, -0.32894721, ...,  0.08196629,\n",
       "        -0.12916289,  0.34466696],\n",
       "       [-0.75960468, -0.4688972 ,  0.80197035, ..., -0.49879355,\n",
       "        -0.42238866,  0.88783695],\n",
       "       ...,\n",
       "       [ 0.68805278, -0.47003264, -0.76090148, ..., -0.44404721,\n",
       "         0.67389389, -0.05768816],\n",
       "       [-0.62228028,  0.57637023, -0.75697214, ..., -0.60348531,\n",
       "         1.18879769,  2.11542062],\n",
       "       [-0.18334221, -0.57890583, -0.89771113, ..., -0.53259158,\n",
       "        -0.22025385,  0.39816732]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enroll_iv.stat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "[-1.5311496  -1.28135981  1.32816947 -0.44887346  0.33056517 -0.06001847\n",
      "  0.10624327 -0.2176878  -0.17061755 -0.11006908  2.44589498 -0.94355654\n",
      " -1.27974243  0.38651779  0.70874621  0.70706126 -0.98676391 -0.40480981\n",
      " -0.65365334 -0.06302094  0.59537732  0.49317728 -0.64532526 -0.79574252\n",
      "  1.09020777]\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "print(enroll_iv.modelset[idx])\n",
    "\n",
    "print(enroll_iv.stat1[idx])\n",
    "\n",
    "print(len(enroll_iv.stat1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
